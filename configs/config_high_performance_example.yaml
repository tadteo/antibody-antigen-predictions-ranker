# High-Performance Configuration Example
# Use this if you have 64+ GB RAM and want maximum I/O speed

data:
  raw_dir: data/raw
  processed_dir: data/processed
  processed_file: processed_data.h5
  splits_dir: data/raw_splits
  manifest_file: data/manifest_new_with_filtered_test_density_with_clipping_500k_maxlen_esm.csv
  feature_transform: true
  feature_centering: true
  batch_size_per_gpu: 4
  
  # ====== HIGH-PERFORMANCE I/O SETTINGS ======
  num_workers: 16              # Increase workers (2x num_GPUs recommended)
  
  # HDF5 file caching - AGGRESSIVE settings for maximum speed
  use_file_cache: true         # Enable file handle caching
  cache_size_mb: 2048          # 2 GB chunk cache per file (large!)
  max_cached_files: 100        # Keep 100 files open per worker (high!)
  # Expected memory: ~16 workers × 100 files × 2 GB = ~3.2 TB max (rarely reached)
  # Typical usage: ~10-20 GB per worker
  # ============================================
  
  samples_per_complex: 5
  use_interchain_ca_distances: true
  use_interchain_pae: true
  use_esm_embeddings: true
  esm_embedding_dim: 320
  use_distance_cutoff: true
  distance_cutoff: 12.0

model:
  input_dim: 4
  aggregator: concat_stats_by_set_size
  phi_hidden_dims: [128, 32, 64]
  rho_hidden_dims: [256, 256, 256]

training:
  adaptive_weight: true
  epochs: 250
  lr: !!float 2e-3
  weight_decay: !!float 1e-5
  smooth_l1_beta: 0.5
  weighted_loss: true
  bucket_balance: false
  num_buckets: 4
  seed: 42
  lr_scheduler_type: "WarmupHoldLinear"
  add_ranking_loss: true
  add_distance_preservation_loss: true
  ranking_loss_type: "one_minus_rho"
  distance_loss_start_epoch: 1
  distance_lambda: 1.0
  distance_lambda_start: 1.0
  distance_lambda_min: 0.2
  distance_lambda_max: 10.0

